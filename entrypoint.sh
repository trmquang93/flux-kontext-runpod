#!/bin/bash

# Enhanced entrypoint script for FLUX.1 Kontext-dev AI Editing Server
# Based on proven production patterns for reliability and performance

# Exit immediately if a command exits with a non-zero status
set -e

echo "ğŸš€ Starting FLUX.1 Kontext-dev AI Editing Server (Enhanced)"
echo "=================================================="

# CUDA ê²€ì‚¬ ë° ì„¤ì • (Enhanced pattern)
echo "ğŸ” Checking CUDA availability..."

# Pythonì„ í†µí•œ CUDA ê²€ì‚¬ (Enhanced pattern)
python_cuda_check() {
    python3 -c "
import torch
try:
    if torch.cuda.is_available():
        print('CUDA_AVAILABLE')
        exit(0)
    else:
        print('CUDA_NOT_AVAILABLE')
        exit(1)
except Exception as e:
    print(f'CUDA_ERROR: {e}')
    exit(2)
" 2>/dev/null
}

# CUDA ìƒíƒœ í™•ì¸
CUDA_STATUS=$(python_cuda_check)
CUDA_EXIT_CODE=$?

case $CUDA_EXIT_CODE in
    0)
        echo "âœ… CUDA is available and working"
        ;;
    1)
        echo "âŒ CUDA is not available"
        echo "This may affect performance. FLUX.1 Kontext will run on CPU."
        ;;
    2)
        echo "âš ï¸ CUDA check encountered an error"
        echo "Proceeding anyway..."
        ;;
esac

# GPU ì •ë³´ ì¶œë ¥ (Enhanced pattern)
if [ $CUDA_EXIT_CODE -eq 0 ]; then
    echo "ğŸ–¥ï¸ GPU Information:"
    python3 -c "
import torch
if torch.cuda.is_available():
    print(f'  Device: {torch.cuda.get_device_name(0)}')
    props = torch.cuda.get_device_properties(0)
    print(f'  Memory: {props.total_memory / 1024**3:.1f}GB')
    print(f'  Compute Capability: {props.major}.{props.minor}')
    print(f'  Multi-processors: {props.multi_processor_count}')
"
fi

# ë„¤íŠ¸ì›Œí¬ ë³¼ë¥¨ ë””ë ‰í† ë¦¬ ìƒì„± (Enhanced pattern)
echo "ğŸ“ Setting up model cache directories..."

# Create cache directories if they don't exist
mkdir -p /runpod-volume/.torch
mkdir -p /runpod-volume/.huggingface
mkdir -p /runpod-volume/.transformers

echo "âœ… Cache directories ready"

# Python ëª¨ë“ˆ ê²€ì‚¬ (Enhanced pattern)
echo "ğŸ” Checking Python dependencies..."

check_python_module() {
    local module_name=$1
    python3 -c "import $module_name" 2>/dev/null
    if [ $? -eq 0 ]; then
        echo "  âœ… $module_name"
    else
        echo "  âŒ $module_name (missing)"
        return 1
    fi
}

# Critical dependencies check
echo "ğŸ§ª Verifying critical dependencies:"
check_python_module "torch" || echo "  ğŸš¨ PyTorch not found!"
check_python_module "diffusers" || echo "  ğŸš¨ Diffusers not found!"
check_python_module "transformers" || echo "  ğŸš¨ Transformers not found!"
check_python_module "PIL" || echo "  ğŸš¨ Pillow not found!"
check_python_module "runpod" || echo "  ğŸš¨ RunPod not found!"

# ë©”ëª¨ë¦¬ ì •ë³´ ì¶œë ¥ (Enhanced pattern)
echo "ğŸ’¾ System Memory Information:"
python3 -c "
import psutil
mem = psutil.virtual_memory()
print(f'  Total: {mem.total / 1024**3:.1f}GB')
print(f'  Available: {mem.available / 1024**3:.1f}GB')
print(f'  Used: {mem.used / 1024**3:.1f}GB ({mem.percent}%)')
"

# HuggingFace í† í° í™•ì¸ (ì„ íƒì )
if [ -n "$HF_TOKEN" ]; then
    echo "ğŸ¤— HuggingFace token found - private models accessible"
else
    echo "â„¹ï¸ No HuggingFace token - using public models only"
fi

# FLUX.1 Kontext specific setup
echo "âš¡ FLUX.1 Kontext-dev Setup:"
echo "  Model: black-forest-labs/FLUX.1-Kontext-dev"
echo "  Task: Text-based image editing"
echo "  Expected memory: ~24GB VRAM (12B parameters)"

# ì„œë²„ ëª¨ë“œ ê²°ì • (Enhanced pattern)
SERVER_MODE=${SERVER_MODE:-runpod}
echo "ğŸ”§ Server mode: $SERVER_MODE"

# í™˜ê²½ë³€ìˆ˜ ê²€ì¦
echo "ğŸ” Environment validation:"
echo "  CUDA_VISIBLE_DEVICES: ${CUDA_VISIBLE_DEVICES:-not set}"
echo "  TORCH_HOME: ${TORCH_HOME:-not set}"
echo "  HF_HOME: ${HF_HOME:-not set}"

case $SERVER_MODE in
    runpod|serverless)
        echo "ğŸš€ Starting RunPod serverless handler..."
        exec python3 runpod_handler.py
        ;;
    fastapi|web)
        echo "ğŸš€ Starting FastAPI web server..."
        exec uvicorn main:app --host 0.0.0.0 --port 8000
        ;;
    debug)
        echo "ğŸ” Starting in debug mode..."
        python3 -c "
import torch
import diffusers
print(f'PyTorch version: {torch.__version__}')
print(f'Diffusers version: {diffusers.__version__}')
if torch.cuda.is_available():
    print(f'CUDA version: {torch.version.cuda}')
    print(f'cuDNN version: {torch.backends.cudnn.version()}')
print('Debug mode - server not started')
"
        ;;
    *)
        echo "âŒ Unknown server mode: $SERVER_MODE"
        echo "Valid modes: runpod, fastapi, debug"
        exit 1
        ;;
esac